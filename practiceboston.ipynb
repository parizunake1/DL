{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "metadata": {
        "id": "iFR2pcxCiBvq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "boston = pd.read_csv('./boston.csv')\n",
        "boston.head(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "BeKlCzU2iYok",
        "outputId": "fe231417-2673-47d0-c5fe-242ab3ca17b6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS  RAD    TAX  \\\n",
              "0   0.00632  18.0   2.31     0  0.538  6.575   65.2  4.0900    1  296.0   \n",
              "1   0.02731   0.0   7.07     0  0.469  6.421   78.9  4.9671    2  242.0   \n",
              "2   0.02729   0.0   7.07     0  0.469  7.185   61.1  4.9671    2  242.0   \n",
              "3   0.03237   0.0   2.18     0  0.458  6.998   45.8  6.0622    3  222.0   \n",
              "4   0.06905   0.0   2.18     0  0.458  7.147   54.2  6.0622    3  222.0   \n",
              "5   0.02985   0.0   2.18     0  0.458  6.430   58.7  6.0622    3  222.0   \n",
              "6   0.08829  12.5   7.87     0  0.524  6.012   66.6  5.5605    5  311.0   \n",
              "7   0.14455  12.5   7.87     0  0.524  6.172   96.1  5.9505    5  311.0   \n",
              "8   0.21124  12.5   7.87     0  0.524  5.631  100.0  6.0821    5  311.0   \n",
              "9   0.17004  12.5   7.87     0  0.524  6.004   85.9  6.5921    5  311.0   \n",
              "10  0.22489  12.5   7.87     0  0.524  6.377   94.3  6.3467    5  311.0   \n",
              "11  0.11747  12.5   7.87     0  0.524  6.009   82.9  6.2267    5  311.0   \n",
              "12  0.09378  12.5   7.87     0  0.524  5.889   39.0  5.4509    5  311.0   \n",
              "13  0.62976   0.0   8.14     0  0.538  5.949   61.8  4.7075    4  307.0   \n",
              "14  0.63796   0.0   8.14     0  0.538  6.096   84.5  4.4619    4  307.0   \n",
              "15  0.62739   0.0   8.14     0  0.538  5.834   56.5  4.4986    4  307.0   \n",
              "16  1.05393   0.0   8.14     0  0.538  5.935   29.3  4.4986    4  307.0   \n",
              "17  0.78420   0.0   8.14     0  0.538  5.990   81.7  4.2579    4  307.0   \n",
              "18  0.80271   0.0   8.14     0  0.538  5.456   36.6  3.7965    4  307.0   \n",
              "19  0.72580   0.0   8.14     0  0.538  5.727   69.5  3.7965    4  307.0   \n",
              "\n",
              "    PTRATIO       B  LSTAT  MEDV  \n",
              "0      15.3  396.90   4.98  24.0  \n",
              "1      17.8  396.90   9.14  21.6  \n",
              "2      17.8  392.83   4.03  34.7  \n",
              "3      18.7  394.63   2.94  33.4  \n",
              "4      18.7  396.90   5.33  36.2  \n",
              "5      18.7  394.12   5.21  28.7  \n",
              "6      15.2  395.60  12.43  22.9  \n",
              "7      15.2  396.90  19.15  27.1  \n",
              "8      15.2  386.63  29.93  16.5  \n",
              "9      15.2  386.71  17.10  18.9  \n",
              "10     15.2  392.52  20.45  15.0  \n",
              "11     15.2  396.90  13.27  18.9  \n",
              "12     15.2  390.50  15.71  21.7  \n",
              "13     21.0  396.90   8.26  20.4  \n",
              "14     21.0  380.02  10.26  18.2  \n",
              "15     21.0  395.62   8.47  19.9  \n",
              "16     21.0  386.85   6.58  23.1  \n",
              "17     21.0  386.75  14.67  17.5  \n",
              "18     21.0  288.99  11.69  20.2  \n",
              "19     21.0  390.95  11.28  18.2  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-de9b1b10-2e77-478f-9c5e-854608b13915\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "      <th>MEDV</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1</td>\n",
              "      <td>296.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "      <td>24.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "      <td>21.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "      <td>34.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "      <td>33.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "      <td>36.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.02985</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.430</td>\n",
              "      <td>58.7</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.12</td>\n",
              "      <td>5.21</td>\n",
              "      <td>28.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.08829</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>6.012</td>\n",
              "      <td>66.6</td>\n",
              "      <td>5.5605</td>\n",
              "      <td>5</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>395.60</td>\n",
              "      <td>12.43</td>\n",
              "      <td>22.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.14455</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>6.172</td>\n",
              "      <td>96.1</td>\n",
              "      <td>5.9505</td>\n",
              "      <td>5</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>396.90</td>\n",
              "      <td>19.15</td>\n",
              "      <td>27.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.21124</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>5.631</td>\n",
              "      <td>100.0</td>\n",
              "      <td>6.0821</td>\n",
              "      <td>5</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>386.63</td>\n",
              "      <td>29.93</td>\n",
              "      <td>16.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.17004</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>6.004</td>\n",
              "      <td>85.9</td>\n",
              "      <td>6.5921</td>\n",
              "      <td>5</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>386.71</td>\n",
              "      <td>17.10</td>\n",
              "      <td>18.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.22489</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>6.377</td>\n",
              "      <td>94.3</td>\n",
              "      <td>6.3467</td>\n",
              "      <td>5</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>392.52</td>\n",
              "      <td>20.45</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.11747</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>6.009</td>\n",
              "      <td>82.9</td>\n",
              "      <td>6.2267</td>\n",
              "      <td>5</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>396.90</td>\n",
              "      <td>13.27</td>\n",
              "      <td>18.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.09378</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>5.889</td>\n",
              "      <td>39.0</td>\n",
              "      <td>5.4509</td>\n",
              "      <td>5</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>390.50</td>\n",
              "      <td>15.71</td>\n",
              "      <td>21.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.62976</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.14</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>5.949</td>\n",
              "      <td>61.8</td>\n",
              "      <td>4.7075</td>\n",
              "      <td>4</td>\n",
              "      <td>307.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>8.26</td>\n",
              "      <td>20.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.63796</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.14</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.096</td>\n",
              "      <td>84.5</td>\n",
              "      <td>4.4619</td>\n",
              "      <td>4</td>\n",
              "      <td>307.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>380.02</td>\n",
              "      <td>10.26</td>\n",
              "      <td>18.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.62739</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.14</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>5.834</td>\n",
              "      <td>56.5</td>\n",
              "      <td>4.4986</td>\n",
              "      <td>4</td>\n",
              "      <td>307.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>395.62</td>\n",
              "      <td>8.47</td>\n",
              "      <td>19.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1.05393</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.14</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>5.935</td>\n",
              "      <td>29.3</td>\n",
              "      <td>4.4986</td>\n",
              "      <td>4</td>\n",
              "      <td>307.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>386.85</td>\n",
              "      <td>6.58</td>\n",
              "      <td>23.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.78420</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.14</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>5.990</td>\n",
              "      <td>81.7</td>\n",
              "      <td>4.2579</td>\n",
              "      <td>4</td>\n",
              "      <td>307.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>386.75</td>\n",
              "      <td>14.67</td>\n",
              "      <td>17.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.80271</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.14</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>5.456</td>\n",
              "      <td>36.6</td>\n",
              "      <td>3.7965</td>\n",
              "      <td>4</td>\n",
              "      <td>307.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>288.99</td>\n",
              "      <td>11.69</td>\n",
              "      <td>20.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.72580</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.14</td>\n",
              "      <td>0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>5.727</td>\n",
              "      <td>69.5</td>\n",
              "      <td>3.7965</td>\n",
              "      <td>4</td>\n",
              "      <td>307.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>390.95</td>\n",
              "      <td>11.28</td>\n",
              "      <td>18.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-de9b1b10-2e77-478f-9c5e-854608b13915')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-de9b1b10-2e77-478f-9c5e-854608b13915 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-de9b1b10-2e77-478f-9c5e-854608b13915');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = boston.loc[:,boston.columns != 'MEDV']\n",
        "y = boston.loc[:,boston.columns == 'MEDV']\n",
        "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size = 0.3, random_state = 123)"
      ],
      "metadata": {
        "id": "cCTIPNeAihV4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(128, input_shape=(13, ),activation='relu',name='dense_1'))\n",
        "model.add(Dense(64,activation='relu',name='dense_2'))\n",
        "model.add(Dense(1,activation='linear',name='dense_output'))\n",
        "\n",
        "model.compile(optimizer='adam',loss='mse',metrics=['mae'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkrQwHyBjMpp",
        "outputId": "76a17b03-e16f-4d7a-a336-2a962ccbe81e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_1 (Dense)             (None, 128)               1792      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_output (Dense)        (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10,113\n",
            "Trainable params: 10,113\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train,y_train,epochs=200,validation_split=0.05,verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFuHDt-bkDT5",
        "outputId": "155c69bb-c69b-48c3-ee04-ff60d1426974"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "11/11 [==============================] - 1s 20ms/step - loss: 1378.7510 - mae: 30.9239 - val_loss: 181.9225 - val_mae: 10.5564\n",
            "Epoch 2/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 307.2800 - mae: 14.3235 - val_loss: 210.8703 - val_mae: 12.5383\n",
            "Epoch 3/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 155.2165 - mae: 10.4574 - val_loss: 156.0249 - val_mae: 9.6816\n",
            "Epoch 4/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 90.7625 - mae: 6.9526 - val_loss: 146.1517 - val_mae: 10.3511\n",
            "Epoch 5/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 68.8065 - mae: 6.2639 - val_loss: 115.0589 - val_mae: 7.4664\n",
            "Epoch 6/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 62.8790 - mae: 5.6796 - val_loss: 99.5979 - val_mae: 7.4568\n",
            "Epoch 7/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 58.3054 - mae: 5.2688 - val_loss: 97.8658 - val_mae: 7.0994\n",
            "Epoch 8/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 57.3738 - mae: 5.4216 - val_loss: 96.2965 - val_mae: 7.2150\n",
            "Epoch 9/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 56.7674 - mae: 5.3022 - val_loss: 94.2690 - val_mae: 7.2399\n",
            "Epoch 10/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 55.3372 - mae: 5.4323 - val_loss: 100.0713 - val_mae: 6.6154\n",
            "Epoch 11/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 57.1595 - mae: 5.3296 - val_loss: 92.8269 - val_mae: 7.4046\n",
            "Epoch 12/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 55.3037 - mae: 5.2435 - val_loss: 91.5096 - val_mae: 7.0663\n",
            "Epoch 13/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 52.6493 - mae: 5.0929 - val_loss: 90.3631 - val_mae: 7.4642\n",
            "Epoch 14/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 54.1007 - mae: 5.2840 - val_loss: 87.4690 - val_mae: 6.8538\n",
            "Epoch 15/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 52.7130 - mae: 5.2423 - val_loss: 100.6516 - val_mae: 6.5264\n",
            "Epoch 16/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 53.2392 - mae: 5.1341 - val_loss: 88.9087 - val_mae: 6.4820\n",
            "Epoch 17/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 50.7540 - mae: 4.9613 - val_loss: 84.2179 - val_mae: 6.7815\n",
            "Epoch 18/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 50.1563 - mae: 5.0301 - val_loss: 85.7267 - val_mae: 6.4170\n",
            "Epoch 19/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 50.3671 - mae: 5.0057 - val_loss: 87.3523 - val_mae: 6.3131\n",
            "Epoch 20/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 49.4706 - mae: 4.9284 - val_loss: 85.2959 - val_mae: 6.1375\n",
            "Epoch 21/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 50.2011 - mae: 5.0369 - val_loss: 79.7998 - val_mae: 6.6551\n",
            "Epoch 22/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 49.4740 - mae: 4.8231 - val_loss: 78.8187 - val_mae: 6.7061\n",
            "Epoch 23/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 48.5470 - mae: 4.9597 - val_loss: 79.2452 - val_mae: 7.0299\n",
            "Epoch 24/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 47.0726 - mae: 4.6451 - val_loss: 80.3604 - val_mae: 7.4422\n",
            "Epoch 25/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 48.9289 - mae: 4.9529 - val_loss: 78.1122 - val_mae: 7.2684\n",
            "Epoch 26/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 47.7550 - mae: 4.8983 - val_loss: 75.4789 - val_mae: 6.7942\n",
            "Epoch 27/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 43.9876 - mae: 4.6024 - val_loss: 74.5312 - val_mae: 6.7814\n",
            "Epoch 28/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 44.5793 - mae: 4.7399 - val_loss: 71.5019 - val_mae: 6.2464\n",
            "Epoch 29/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 44.3843 - mae: 4.6956 - val_loss: 69.4504 - val_mae: 6.2209\n",
            "Epoch 30/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 48.9154 - mae: 5.0720 - val_loss: 69.3208 - val_mae: 6.2592\n",
            "Epoch 31/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 42.3793 - mae: 4.6211 - val_loss: 72.5063 - val_mae: 5.6753\n",
            "Epoch 32/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 41.8634 - mae: 4.4640 - val_loss: 69.6320 - val_mae: 5.9097\n",
            "Epoch 33/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 42.0093 - mae: 4.7164 - val_loss: 71.7514 - val_mae: 5.4180\n",
            "Epoch 34/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 40.1143 - mae: 4.4867 - val_loss: 68.8736 - val_mae: 5.6960\n",
            "Epoch 35/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 41.4846 - mae: 4.6785 - val_loss: 86.5219 - val_mae: 5.5219\n",
            "Epoch 36/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 45.9386 - mae: 4.8891 - val_loss: 88.6681 - val_mae: 5.7558\n",
            "Epoch 37/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 40.3043 - mae: 4.5663 - val_loss: 68.1919 - val_mae: 5.2466\n",
            "Epoch 38/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 37.3736 - mae: 4.2632 - val_loss: 67.7884 - val_mae: 5.2686\n",
            "Epoch 39/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 37.3095 - mae: 4.2518 - val_loss: 67.2344 - val_mae: 5.3969\n",
            "Epoch 40/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 36.8993 - mae: 4.3467 - val_loss: 68.0414 - val_mae: 5.2896\n",
            "Epoch 41/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 36.7101 - mae: 4.1212 - val_loss: 74.3995 - val_mae: 5.2769\n",
            "Epoch 42/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 40.1814 - mae: 4.4846 - val_loss: 67.5107 - val_mae: 6.3971\n",
            "Epoch 43/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 40.0187 - mae: 4.5881 - val_loss: 66.8871 - val_mae: 6.7947\n",
            "Epoch 44/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 36.7891 - mae: 4.2821 - val_loss: 64.8440 - val_mae: 5.4782\n",
            "Epoch 45/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 33.7139 - mae: 4.0686 - val_loss: 61.9716 - val_mae: 5.5199\n",
            "Epoch 46/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 34.4197 - mae: 4.0876 - val_loss: 72.2011 - val_mae: 5.6154\n",
            "Epoch 47/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 35.7318 - mae: 4.2457 - val_loss: 60.7708 - val_mae: 5.8984\n",
            "Epoch 48/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 37.0570 - mae: 4.3748 - val_loss: 66.7810 - val_mae: 5.4246\n",
            "Epoch 49/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 33.2486 - mae: 4.0365 - val_loss: 62.8700 - val_mae: 5.4121\n",
            "Epoch 50/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 32.4000 - mae: 4.0093 - val_loss: 67.0008 - val_mae: 5.4860\n",
            "Epoch 51/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 31.9049 - mae: 4.0045 - val_loss: 62.8591 - val_mae: 5.7715\n",
            "Epoch 52/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 31.5292 - mae: 4.0742 - val_loss: 67.7624 - val_mae: 5.4565\n",
            "Epoch 53/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 31.1339 - mae: 3.9112 - val_loss: 67.2602 - val_mae: 5.5806\n",
            "Epoch 54/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 33.8805 - mae: 4.1891 - val_loss: 77.7654 - val_mae: 5.6050\n",
            "Epoch 55/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 34.7930 - mae: 4.2265 - val_loss: 60.3345 - val_mae: 6.1184\n",
            "Epoch 56/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 32.6336 - mae: 4.1765 - val_loss: 73.9627 - val_mae: 5.4923\n",
            "Epoch 57/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 33.2865 - mae: 4.1939 - val_loss: 71.4387 - val_mae: 5.5308\n",
            "Epoch 58/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 32.8853 - mae: 4.1802 - val_loss: 61.6927 - val_mae: 5.7147\n",
            "Epoch 59/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 30.8855 - mae: 3.9910 - val_loss: 74.3333 - val_mae: 5.5469\n",
            "Epoch 60/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 30.5115 - mae: 3.9369 - val_loss: 68.3297 - val_mae: 5.6293\n",
            "Epoch 61/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 28.6271 - mae: 3.8543 - val_loss: 61.9968 - val_mae: 6.0475\n",
            "Epoch 62/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 28.6953 - mae: 3.9415 - val_loss: 67.0160 - val_mae: 5.5360\n",
            "Epoch 63/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 28.5884 - mae: 3.8649 - val_loss: 65.0182 - val_mae: 5.5443\n",
            "Epoch 64/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 27.1612 - mae: 3.7310 - val_loss: 71.9131 - val_mae: 5.5776\n",
            "Epoch 65/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 27.9089 - mae: 3.7813 - val_loss: 62.0171 - val_mae: 5.6808\n",
            "Epoch 66/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 27.5403 - mae: 3.7787 - val_loss: 77.9945 - val_mae: 5.5195\n",
            "Epoch 67/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 26.9988 - mae: 3.7455 - val_loss: 66.2270 - val_mae: 5.4267\n",
            "Epoch 68/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 27.9014 - mae: 3.9164 - val_loss: 84.8350 - val_mae: 5.6990\n",
            "Epoch 69/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 28.7952 - mae: 3.9229 - val_loss: 66.8618 - val_mae: 5.4802\n",
            "Epoch 70/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 27.7333 - mae: 3.8226 - val_loss: 67.8119 - val_mae: 5.4222\n",
            "Epoch 71/200\n",
            "11/11 [==============================] - 0s 10ms/step - loss: 26.8921 - mae: 3.7321 - val_loss: 69.1838 - val_mae: 5.6672\n",
            "Epoch 72/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 28.8134 - mae: 4.1519 - val_loss: 64.2038 - val_mae: 5.5814\n",
            "Epoch 73/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 27.5046 - mae: 3.7052 - val_loss: 64.2406 - val_mae: 5.7333\n",
            "Epoch 74/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 27.2037 - mae: 3.8779 - val_loss: 82.3068 - val_mae: 5.6823\n",
            "Epoch 75/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 27.1042 - mae: 3.7447 - val_loss: 73.1849 - val_mae: 5.3961\n",
            "Epoch 76/200\n",
            "11/11 [==============================] - 0s 11ms/step - loss: 25.0632 - mae: 3.6666 - val_loss: 78.7679 - val_mae: 5.4032\n",
            "Epoch 77/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 24.7786 - mae: 3.6625 - val_loss: 62.0482 - val_mae: 5.7774\n",
            "Epoch 78/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 27.6638 - mae: 3.9197 - val_loss: 110.0014 - val_mae: 7.4228\n",
            "Epoch 79/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 33.0982 - mae: 4.3224 - val_loss: 68.7228 - val_mae: 7.0511\n",
            "Epoch 80/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 31.0663 - mae: 4.3813 - val_loss: 78.3899 - val_mae: 5.4530\n",
            "Epoch 81/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 26.5061 - mae: 3.7742 - val_loss: 71.9116 - val_mae: 5.4878\n",
            "Epoch 82/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 25.8758 - mae: 3.7289 - val_loss: 78.4933 - val_mae: 5.6680\n",
            "Epoch 83/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 26.8693 - mae: 3.8637 - val_loss: 67.6940 - val_mae: 6.6262\n",
            "Epoch 84/200\n",
            "11/11 [==============================] - 0s 9ms/step - loss: 30.2803 - mae: 4.0815 - val_loss: 87.8291 - val_mae: 6.0687\n",
            "Epoch 85/200\n",
            "11/11 [==============================] - 0s 10ms/step - loss: 24.2600 - mae: 3.7009 - val_loss: 71.7404 - val_mae: 5.2899\n",
            "Epoch 86/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 23.5112 - mae: 3.5213 - val_loss: 71.1121 - val_mae: 5.3817\n",
            "Epoch 87/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 24.4764 - mae: 3.7402 - val_loss: 81.1255 - val_mae: 5.5093\n",
            "Epoch 88/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 25.5497 - mae: 3.8019 - val_loss: 70.8703 - val_mae: 5.4344\n",
            "Epoch 89/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 26.1399 - mae: 3.7474 - val_loss: 67.7404 - val_mae: 5.3024\n",
            "Epoch 90/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 23.0756 - mae: 3.4653 - val_loss: 70.4331 - val_mae: 5.8159\n",
            "Epoch 91/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 24.2682 - mae: 3.7076 - val_loss: 69.8689 - val_mae: 5.4296\n",
            "Epoch 92/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 24.1951 - mae: 3.6180 - val_loss: 63.3448 - val_mae: 5.4948\n",
            "Epoch 93/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 25.9821 - mae: 3.7846 - val_loss: 70.3211 - val_mae: 5.2519\n",
            "Epoch 94/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 24.9490 - mae: 3.8316 - val_loss: 65.9786 - val_mae: 5.6383\n",
            "Epoch 95/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 24.7741 - mae: 3.7152 - val_loss: 66.7939 - val_mae: 5.3316\n",
            "Epoch 96/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 25.0465 - mae: 3.8328 - val_loss: 75.2394 - val_mae: 5.2453\n",
            "Epoch 97/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 23.7033 - mae: 3.5269 - val_loss: 61.9269 - val_mae: 5.3140\n",
            "Epoch 98/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 22.8065 - mae: 3.4914 - val_loss: 68.0655 - val_mae: 5.1837\n",
            "Epoch 99/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 22.3755 - mae: 3.4830 - val_loss: 71.2859 - val_mae: 5.0360\n",
            "Epoch 100/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 24.9607 - mae: 3.6214 - val_loss: 63.9455 - val_mae: 6.5857\n",
            "Epoch 101/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 28.1532 - mae: 4.1313 - val_loss: 72.4463 - val_mae: 5.0825\n",
            "Epoch 102/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 25.9175 - mae: 3.8244 - val_loss: 70.7315 - val_mae: 5.0745\n",
            "Epoch 103/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 22.9409 - mae: 3.4715 - val_loss: 60.5356 - val_mae: 5.5376\n",
            "Epoch 104/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 25.2437 - mae: 3.7441 - val_loss: 75.4376 - val_mae: 5.2545\n",
            "Epoch 105/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 26.0134 - mae: 3.9168 - val_loss: 75.1185 - val_mae: 5.2431\n",
            "Epoch 106/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 21.8029 - mae: 3.4333 - val_loss: 64.0489 - val_mae: 5.1463\n",
            "Epoch 107/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 20.8635 - mae: 3.3442 - val_loss: 63.1579 - val_mae: 5.4372\n",
            "Epoch 108/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 22.0644 - mae: 3.5636 - val_loss: 67.2370 - val_mae: 5.1935\n",
            "Epoch 109/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 24.0816 - mae: 3.6162 - val_loss: 74.7868 - val_mae: 5.4908\n",
            "Epoch 110/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 26.1430 - mae: 3.9144 - val_loss: 60.4017 - val_mae: 5.3319\n",
            "Epoch 111/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 23.8602 - mae: 3.4948 - val_loss: 64.4913 - val_mae: 5.1298\n",
            "Epoch 112/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 21.4880 - mae: 3.4246 - val_loss: 73.0080 - val_mae: 5.4344\n",
            "Epoch 113/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 22.1589 - mae: 3.4851 - val_loss: 61.5554 - val_mae: 5.0458\n",
            "Epoch 114/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 22.0216 - mae: 3.5295 - val_loss: 84.0856 - val_mae: 6.1879\n",
            "Epoch 115/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 25.4160 - mae: 3.7272 - val_loss: 61.3842 - val_mae: 5.2390\n",
            "Epoch 116/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 26.1522 - mae: 3.8778 - val_loss: 63.3684 - val_mae: 5.3131\n",
            "Epoch 117/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 34.6601 - mae: 4.7329 - val_loss: 97.2138 - val_mae: 6.9179\n",
            "Epoch 118/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 28.2162 - mae: 4.0671 - val_loss: 56.5428 - val_mae: 5.4574\n",
            "Epoch 119/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 23.2973 - mae: 3.5903 - val_loss: 68.4786 - val_mae: 5.1418\n",
            "Epoch 120/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 20.1888 - mae: 3.3661 - val_loss: 66.5721 - val_mae: 4.8874\n",
            "Epoch 121/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 20.7869 - mae: 3.3685 - val_loss: 61.0438 - val_mae: 5.3336\n",
            "Epoch 122/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 19.6792 - mae: 3.2487 - val_loss: 58.0771 - val_mae: 5.1329\n",
            "Epoch 123/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 20.8016 - mae: 3.5219 - val_loss: 68.4952 - val_mae: 5.0617\n",
            "Epoch 124/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 24.7262 - mae: 3.7975 - val_loss: 92.4524 - val_mae: 6.7261\n",
            "Epoch 125/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 25.9531 - mae: 3.8949 - val_loss: 63.1660 - val_mae: 4.9063\n",
            "Epoch 126/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 29.2994 - mae: 4.1535 - val_loss: 55.0826 - val_mae: 5.4429\n",
            "Epoch 127/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 28.2413 - mae: 4.0637 - val_loss: 55.7645 - val_mae: 5.1942\n",
            "Epoch 128/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 22.5959 - mae: 3.6933 - val_loss: 61.2726 - val_mae: 5.0333\n",
            "Epoch 129/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 20.5245 - mae: 3.2969 - val_loss: 54.1225 - val_mae: 5.4995\n",
            "Epoch 130/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 31.8761 - mae: 4.4147 - val_loss: 106.1430 - val_mae: 7.9166\n",
            "Epoch 131/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 42.7587 - mae: 5.1762 - val_loss: 54.8973 - val_mae: 5.1666\n",
            "Epoch 132/200\n",
            "11/11 [==============================] - 0s 8ms/step - loss: 22.4289 - mae: 3.5678 - val_loss: 82.7654 - val_mae: 6.1225\n",
            "Epoch 133/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 25.4109 - mae: 3.8155 - val_loss: 55.8551 - val_mae: 5.7094\n",
            "Epoch 134/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 25.9261 - mae: 3.9329 - val_loss: 64.5425 - val_mae: 4.9816\n",
            "Epoch 135/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 26.0722 - mae: 3.9733 - val_loss: 65.1866 - val_mae: 5.1324\n",
            "Epoch 136/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 20.0834 - mae: 3.3002 - val_loss: 51.3022 - val_mae: 4.8369\n",
            "Epoch 137/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 18.7468 - mae: 3.1701 - val_loss: 59.2088 - val_mae: 4.8812\n",
            "Epoch 138/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 18.5204 - mae: 3.2167 - val_loss: 56.9010 - val_mae: 5.0201\n",
            "Epoch 139/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 20.0844 - mae: 3.4265 - val_loss: 74.6096 - val_mae: 5.7588\n",
            "Epoch 140/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 20.4867 - mae: 3.4406 - val_loss: 58.4762 - val_mae: 4.8423\n",
            "Epoch 141/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 21.0110 - mae: 3.4944 - val_loss: 52.7260 - val_mae: 5.7117\n",
            "Epoch 142/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 22.1878 - mae: 3.5915 - val_loss: 63.7545 - val_mae: 5.1215\n",
            "Epoch 143/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 21.8007 - mae: 3.5762 - val_loss: 51.7152 - val_mae: 4.6373\n",
            "Epoch 144/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 23.4849 - mae: 3.5506 - val_loss: 51.8853 - val_mae: 5.1384\n",
            "Epoch 145/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 21.9886 - mae: 3.6005 - val_loss: 61.1741 - val_mae: 4.9232\n",
            "Epoch 146/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 17.9806 - mae: 3.1830 - val_loss: 61.5713 - val_mae: 4.9766\n",
            "Epoch 147/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 18.9818 - mae: 3.1910 - val_loss: 52.6659 - val_mae: 5.0443\n",
            "Epoch 148/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 19.6530 - mae: 3.4190 - val_loss: 55.0626 - val_mae: 4.6585\n",
            "Epoch 149/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 19.0711 - mae: 3.2879 - val_loss: 52.7439 - val_mae: 4.5153\n",
            "Epoch 150/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 20.3883 - mae: 3.5045 - val_loss: 81.0766 - val_mae: 6.4918\n",
            "Epoch 151/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 34.7383 - mae: 4.5133 - val_loss: 61.1105 - val_mae: 6.6629\n",
            "Epoch 152/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 37.6391 - mae: 4.8870 - val_loss: 97.9318 - val_mae: 7.6392\n",
            "Epoch 153/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 31.7371 - mae: 4.4102 - val_loss: 42.0993 - val_mae: 4.5807\n",
            "Epoch 154/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 19.4633 - mae: 3.3058 - val_loss: 42.8324 - val_mae: 4.3171\n",
            "Epoch 155/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 18.8381 - mae: 3.1243 - val_loss: 40.6158 - val_mae: 4.8724\n",
            "Epoch 156/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 22.5383 - mae: 3.7192 - val_loss: 58.7182 - val_mae: 5.3357\n",
            "Epoch 157/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 18.4681 - mae: 3.1810 - val_loss: 46.3777 - val_mae: 4.3535\n",
            "Epoch 158/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 17.0720 - mae: 3.0453 - val_loss: 44.2035 - val_mae: 4.2431\n",
            "Epoch 159/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 16.7988 - mae: 3.1502 - val_loss: 53.9947 - val_mae: 4.6329\n",
            "Epoch 160/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 16.9904 - mae: 2.9546 - val_loss: 45.8943 - val_mae: 4.9739\n",
            "Epoch 161/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 18.0400 - mae: 3.2053 - val_loss: 53.1054 - val_mae: 4.4045\n",
            "Epoch 162/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 16.3809 - mae: 2.9601 - val_loss: 44.3195 - val_mae: 4.7748\n",
            "Epoch 163/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 17.9480 - mae: 3.2306 - val_loss: 49.5612 - val_mae: 4.6915\n",
            "Epoch 164/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 17.8252 - mae: 3.1924 - val_loss: 47.2809 - val_mae: 4.2059\n",
            "Epoch 165/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 15.5547 - mae: 2.9440 - val_loss: 62.8758 - val_mae: 5.5302\n",
            "Epoch 166/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 17.9702 - mae: 3.0931 - val_loss: 44.8262 - val_mae: 4.1573\n",
            "Epoch 167/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 19.7046 - mae: 3.2595 - val_loss: 44.3483 - val_mae: 5.2772\n",
            "Epoch 168/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 19.7270 - mae: 3.4826 - val_loss: 49.4847 - val_mae: 4.5972\n",
            "Epoch 169/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 16.6543 - mae: 2.9863 - val_loss: 45.8178 - val_mae: 4.5307\n",
            "Epoch 170/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 17.2071 - mae: 3.1765 - val_loss: 58.4387 - val_mae: 4.9719\n",
            "Epoch 171/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 18.5926 - mae: 3.2383 - val_loss: 48.0155 - val_mae: 4.2167\n",
            "Epoch 172/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 16.2346 - mae: 3.0138 - val_loss: 43.5610 - val_mae: 4.0963\n",
            "Epoch 173/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 16.9297 - mae: 3.0339 - val_loss: 57.3600 - val_mae: 4.9419\n",
            "Epoch 174/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 17.3916 - mae: 3.1800 - val_loss: 44.9206 - val_mae: 4.4912\n",
            "Epoch 175/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 15.8673 - mae: 2.9550 - val_loss: 48.4757 - val_mae: 4.4679\n",
            "Epoch 176/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 15.1685 - mae: 2.8719 - val_loss: 43.9136 - val_mae: 4.4370\n",
            "Epoch 177/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 15.3457 - mae: 2.9316 - val_loss: 45.5104 - val_mae: 4.1128\n",
            "Epoch 178/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 16.6133 - mae: 3.1017 - val_loss: 46.3040 - val_mae: 4.0640\n",
            "Epoch 179/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 18.7666 - mae: 3.2550 - val_loss: 41.4819 - val_mae: 4.1935\n",
            "Epoch 180/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 15.5974 - mae: 3.0103 - val_loss: 46.2466 - val_mae: 4.2770\n",
            "Epoch 181/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 14.3447 - mae: 2.8221 - val_loss: 40.8830 - val_mae: 4.0682\n",
            "Epoch 182/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 15.1655 - mae: 2.9001 - val_loss: 46.4750 - val_mae: 4.2186\n",
            "Epoch 183/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 17.0078 - mae: 3.0781 - val_loss: 45.4825 - val_mae: 4.2107\n",
            "Epoch 184/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 16.5574 - mae: 3.1212 - val_loss: 49.1682 - val_mae: 4.3139\n",
            "Epoch 185/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 17.2500 - mae: 3.1066 - val_loss: 38.1029 - val_mae: 4.2449\n",
            "Epoch 186/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 16.7156 - mae: 3.0546 - val_loss: 45.8219 - val_mae: 4.2718\n",
            "Epoch 187/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 13.8544 - mae: 2.7507 - val_loss: 42.0686 - val_mae: 3.7732\n",
            "Epoch 188/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 14.5154 - mae: 2.8024 - val_loss: 39.9355 - val_mae: 3.8546\n",
            "Epoch 189/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 16.1895 - mae: 3.0915 - val_loss: 67.4317 - val_mae: 6.0772\n",
            "Epoch 190/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 23.0244 - mae: 3.7527 - val_loss: 37.5655 - val_mae: 4.1232\n",
            "Epoch 191/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 15.8965 - mae: 2.9743 - val_loss: 37.6980 - val_mae: 4.4964\n",
            "Epoch 192/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 18.5522 - mae: 3.3619 - val_loss: 44.4157 - val_mae: 4.3275\n",
            "Epoch 193/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 15.1218 - mae: 2.9190 - val_loss: 39.4942 - val_mae: 4.1305\n",
            "Epoch 194/200\n",
            "11/11 [==============================] - 0s 7ms/step - loss: 15.6262 - mae: 2.9753 - val_loss: 40.6943 - val_mae: 4.0485\n",
            "Epoch 195/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 15.9113 - mae: 2.9305 - val_loss: 41.1799 - val_mae: 3.9244\n",
            "Epoch 196/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 15.7749 - mae: 2.9170 - val_loss: 55.3428 - val_mae: 5.0335\n",
            "Epoch 197/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 15.0634 - mae: 2.9067 - val_loss: 38.8854 - val_mae: 3.9665\n",
            "Epoch 198/200\n",
            "11/11 [==============================] - 0s 6ms/step - loss: 17.2522 - mae: 3.0615 - val_loss: 37.7216 - val_mae: 4.3375\n",
            "Epoch 199/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 16.6758 - mae: 3.1839 - val_loss: 44.8565 - val_mae: 4.2844\n",
            "Epoch 200/200\n",
            "11/11 [==============================] - 0s 5ms/step - loss: 15.1225 - mae: 2.8706 - val_loss: 32.9453 - val_mae: 3.6765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mse_nn, mae_nn = model.evaluate(X_test, y_test)\n",
        "\n",
        "print('mean sq error on test data: ', mse_nn)\n",
        "print('mean ab error on test data: ', mae_nn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Hd8M_cqkeso",
        "outputId": "cf0d58fe-4e95-4544-c5d4-05a638a63afd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 0s 3ms/step - loss: 19.4692 - mae: 3.4100\n",
            "mean sq error on test data:  19.469234466552734\n",
            "mean ab error on test data:  3.4100422859191895\n"
          ]
        }
      ]
    }
  ]
}